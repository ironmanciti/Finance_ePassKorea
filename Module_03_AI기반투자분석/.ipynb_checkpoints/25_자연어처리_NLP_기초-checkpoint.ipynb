{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af41b21",
   "metadata": {},
   "source": [
    "# 25차시: 자연어 처리(NLP) 기초 - 텍스트 데이터 전처리\n",
    "\n",
    "## 학습 목표\n",
    "- 자연어 처리(NLP)의 기본 개념 이해\n",
    "- 텍스트 전처리 (토큰화, 정제) 방법 학습\n",
    "- TF-IDF, Word2Vec 등 텍스트 표현 방법 이해\n",
    "\n",
    "## 학습 내용\n",
    "1. NLP란 무엇인가?\n",
    "2. 텍스트 전처리\n",
    "3. 토큰화 (Tokenization)\n",
    "4. TF-IDF\n",
    "5. Word2Vec 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca73a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5421c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed0667",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. NLP란 무엇인가?\n",
    "\n",
    "### 정의\n",
    "**자연어 처리(Natural Language Processing, NLP)**: 컴퓨터가 인간의 언어를 이해하고 처리하는 기술\n",
    "\n",
    "### 금융에서의 NLP 활용\n",
    "| 활용 분야 | 설명 | 예시 |\n",
    "|-----------|------|------|\n",
    "| 감성 분석 | 텍스트의 긍/부정 판단 | 뉴스 감성 → 주가 영향 예측 |\n",
    "| 정보 추출 | 핵심 정보 자동 추출 | 재무제표에서 수치 추출 |\n",
    "| 텍스트 분류 | 문서 자동 분류 | 공시 유형 분류 |\n",
    "| 요약 | 긴 문서 요약 | 애널리스트 리포트 요약 |\n",
    "\n",
    "### NLP 파이프라인\n",
    "```\n",
    "원본 텍스트 → 전처리 → 토큰화 → 벡터화 → 모델 학습/예측\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37f151",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 텍스트 전처리\n",
    "\n",
    "### 왜 전처리가 필요한가?\n",
    "- 텍스트는 노이즈가 많음 (특수문자, 불용어 등)\n",
    "- 같은 의미의 다른 표현 통일 필요\n",
    "- 모델 성능 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a165a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 예시 금융 뉴스 텍스트\n",
    "sample_texts = [\n",
    "    \"삼성전자, 3분기 실적 발표... 영업이익 10조원 돌파!!!\",\n",
    "    \"코스피 상승세 지속, 외국인 순매수 2000억원\",\n",
    "    \"미국 연준(Fed), 금리 0.25%p 인상 결정\",\n",
    "    \"[속보] SK하이닉스 반도체 수출 호조... 주가 5% 급등\",\n",
    "    \"원/달러 환율 1350원 돌파, 경제 불확실성 확대\"\n",
    "]\n",
    "\n",
    "print(\"[원본 텍스트]\")\n",
    "print(\"=\" * 60)\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"{i}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21eb7a3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    \"\"\"텍스트 전처리 함수\"\"\"\n",
    "    # 1. 소문자 변환 (영어의 경우)\n",
    "    text = text.lower() if text.isascii() else text\n",
    "    \n",
    "    # 2. 특수문자 제거 (한글, 영문, 숫자, 공백만 유지)\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # 3. 연속 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 4. 앞뒤 공백 제거\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 정제 적용\n",
    "cleaned_texts = [clean_text(text) for text in sample_texts]\n",
    "\n",
    "print(\"[정제 후 텍스트]\")\n",
    "print(\"=\" * 60)\n",
    "for i, (orig, clean) in enumerate(zip(sample_texts, cleaned_texts), 1):\n",
    "    print(f\"{i}. 원본: {orig}\")\n",
    "    print(f\"   정제: {clean}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6063fe41",
   "metadata": {},
   "source": [
    "### 주요 전처리 기법\n",
    "| 기법 | 설명 | 예시 |\n",
    "|------|------|------|\n",
    "| 소문자 변환 | 대소문자 통일 | \"Apple\" → \"apple\" |\n",
    "| 특수문자 제거 | 노이즈 제거 | \"10%↑\" → \"10\" |\n",
    "| 불용어 제거 | 의미 없는 단어 제거 | \"의\", \"이\", \"는\" 제거 |\n",
    "| 어간 추출 | 단어 원형 추출 | \"running\" → \"run\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c0feb2",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 토큰화 (Tokenization)\n",
    "\n",
    "### 정의\n",
    "**토큰화**: 텍스트를 작은 단위(토큰)로 분리하는 작업\n",
    "\n",
    "### 토큰 단위\n",
    "- 단어 (Word): 가장 일반적\n",
    "- 문장 (Sentence)\n",
    "- 형태소 (Morpheme): 한국어에서 중요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cb1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 공백 기반 토큰화\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"공백 기반 토큰화\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "# 적용\n",
    "for text in cleaned_texts[:3]:\n",
    "    tokens = simple_tokenize(text)\n",
    "    print(f\"텍스트: {text}\")\n",
    "    print(f\"토큰: {tokens}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 형태소 분석기 (KoNLPy)\n",
    "print(\"[한국어 형태소 분석]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from konlpy.tag import Okt\n",
    "    \n",
    "    okt = Okt()\n",
    "    \n",
    "    # 형태소 분석 예시\n",
    "    sample = \"삼성전자가 3분기 영업이익 10조원을 돌파했다\"\n",
    "    \n",
    "    print(f\"원본: {sample}\")\n",
    "    print(f\"\\n형태소 분석: {okt.morphs(sample)}\")\n",
    "    print(f\"명사 추출: {okt.nouns(sample)}\")\n",
    "    print(f\"품사 태깅: {okt.pos(sample)}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"KoNLPy가 설치되지 않았습니다.\")\n",
    "    print(\"Colab에서는 아래 명령어로 설치할 수 있습니다:\")\n",
    "    print(\"  !pip install konlpy\")\n",
    "    print(\"\\n간단한 공백 기반 토큰화로 대체합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd041d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 처리\n",
    "korean_stopwords = ['의', '가', '이', '은', '는', '에', '를', '을', '와', '과', \n",
    "                    '도', '로', '에서', '으로', '한', '하다', '있다', '되다']\n",
    "\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    \"\"\"불용어 제거\"\"\"\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "# 예시\n",
    "sample_tokens = ['삼성전자', '가', '3분기', '영업이익', '10조원', '을', '돌파', '했다']\n",
    "filtered = remove_stopwords(sample_tokens, korean_stopwords)\n",
    "\n",
    "print(\"[불용어 제거]\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"원본 토큰: {sample_tokens}\")\n",
    "print(f\"제거 후: {filtered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b55536f",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. TF-IDF\n",
    "\n",
    "### 정의\n",
    "**TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "- 단어의 중요도를 수치로 표현\n",
    "- 문서에서 자주 등장하지만, 다른 문서에서는 드문 단어에 높은 가중치\n",
    "\n",
    "### 수식\n",
    "- **TF**: 해당 문서에서 단어 등장 빈도\n",
    "- **IDF**: 전체 문서 중 해당 단어가 등장한 문서의 역빈도\n",
    "- **TF-IDF = TF × IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a357263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# 예시 문서\n",
    "documents = [\n",
    "    \"삼성전자 반도체 실적 호조\",\n",
    "    \"반도체 시장 성장세 지속\",\n",
    "    \"삼성전자 스마트폰 점유율 상승\",\n",
    "    \"애플 아이폰 신제품 출시\",\n",
    "    \"삼성전자 애플 경쟁 심화\"\n",
    "]\n",
    "\n",
    "print(\"[TF-IDF 분석]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(documents)\n",
    "\n",
    "# 결과를 DataFrame으로\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(\"TF-IDF 행렬:\")\n",
    "display(df_tfidf.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696d010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 시각화 (첫 번째 문서)\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "doc_idx = 0\n",
    "tfidf_scores = df_tfidf.iloc[doc_idx].sort_values(ascending=True)\n",
    "tfidf_scores = tfidf_scores[tfidf_scores > 0]\n",
    "\n",
    "ax.barh(tfidf_scores.index, tfidf_scores.values, color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('TF-IDF Score')\n",
    "ax.set_title(f'Document {doc_idx + 1}: \"{documents[doc_idx]}\"')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도 (Count Vectorizer) 비교\n",
    "count_vec = CountVectorizer()\n",
    "count_matrix = count_vec.fit_transform(documents)\n",
    "\n",
    "df_count = pd.DataFrame(\n",
    "    count_matrix.toarray(), \n",
    "    columns=count_vec.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(\"[단어 빈도 vs TF-IDF 비교]\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n단어 '삼성전자':\")\n",
    "print(f\"  빈도 (Count): {df_count['삼성전자'].tolist()}\")\n",
    "print(f\"  TF-IDF: {df_tfidf['삼성전자'].round(2).tolist()}\")\n",
    "print(\"\\n단어 '반도체':\")\n",
    "print(f\"  빈도 (Count): {df_count['반도체'].tolist()}\")\n",
    "print(f\"  TF-IDF: {df_tfidf['반도체'].round(2).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f46e0",
   "metadata": {},
   "source": [
    "### TF-IDF 해석\n",
    "- 높은 TF-IDF: 해당 문서에서 중요한 단어\n",
    "- 낮은 TF-IDF: 흔하거나 중요하지 않은 단어\n",
    "- 금융 활용: 뉴스에서 핵심 키워드 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565db1fa",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Word2Vec 개념\n",
    "\n",
    "### Word2Vec이란?\n",
    "- 단어를 고정 크기의 벡터(숫자 배열)로 변환\n",
    "- **의미적으로 유사한 단어는 유사한 벡터**\n",
    "- 예: \"왕\" - \"남자\" + \"여자\" ≈ \"여왕\"\n",
    "\n",
    "### TF-IDF vs Word2Vec\n",
    "| 구분 | TF-IDF | Word2Vec |\n",
    "|------|--------|----------|\n",
    "| 표현 | 희소 벡터 (Sparse) | 밀집 벡터 (Dense) |\n",
    "| 의미 | 빈도 기반 | 문맥 기반 |\n",
    "| 유사도 | 단어 독립 | 의미적 유사도 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd9e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 개념 시각화 (2D로 단순화)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 가상의 단어 벡터 (실제는 Word2Vec으로 학습)\n",
    "word_vectors = {\n",
    "    '삼성전자': [0.8, 0.2],\n",
    "    'SK하이닉스': [0.7, 0.3],\n",
    "    '반도체': [0.75, 0.25],\n",
    "    '애플': [0.6, -0.3],\n",
    "    '아이폰': [0.5, -0.35],\n",
    "    '스마트폰': [0.55, -0.1],\n",
    "    '코스피': [-0.2, 0.8],\n",
    "    '코스닥': [-0.3, 0.75],\n",
    "    '주가': [-0.1, 0.7],\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for word, vec in word_vectors.items():\n",
    "    ax.scatter(vec[0], vec[1], s=100, alpha=0.7)\n",
    "    ax.annotate(word, (vec[0], vec[1]), fontsize=11, \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_title('Word Vectors in 2D Space (Conceptual)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"[Word2Vec 특징]\")\n",
    "print(\"=\" * 60)\n",
    "print(\"- 유사한 단어는 가까운 위치에 배치\")\n",
    "print(\"- '삼성전자', 'SK하이닉스', '반도체'가 가까움\")\n",
    "print(\"- '코스피', '코스닥', '주가'가 가까움\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de54728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim Word2Vec 예시 (설치 필요)\n",
    "print(\"[Word2Vec 사용 예시 (개념)]\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "# Gensim 라이브러리 사용\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 문장 리스트 (토큰화된 상태)\n",
    "sentences = [\n",
    "    ['삼성전자', '반도체', '실적', '호조'],\n",
    "    ['SK하이닉스', '반도체', '수출', '증가'],\n",
    "    ['코스피', '상승', '외국인', '순매수'],\n",
    "    ...\n",
    "]\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# 유사 단어 찾기\n",
    "similar = model.wv.most_similar('삼성전자', topn=5)\n",
    "print(similar)\n",
    "# [('SK하이닉스', 0.92), ('반도체', 0.88), ...]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f853b",
   "metadata": {},
   "source": [
    "---\n",
    "## 학습 정리\n",
    "\n",
    "### 1. NLP 파이프라인\n",
    "```\n",
    "텍스트 → 전처리 → 토큰화 → 벡터화 → 분석/모델\n",
    "```\n",
    "\n",
    "### 2. 텍스트 전처리\n",
    "```python\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "```\n",
    "\n",
    "### 3. 토큰화\n",
    "```python\n",
    "# 한국어\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "tokens = okt.morphs(text)  # 형태소 분석\n",
    "nouns = okt.nouns(text)    # 명사 추출\n",
    "```\n",
    "\n",
    "### 4. TF-IDF\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "matrix = tfidf.fit_transform(documents)\n",
    "```\n",
    "\n",
    "### 5. 텍스트 표현 방법\n",
    "| 방법 | 특징 | 활용 |\n",
    "|------|------|------|\n",
    "| BoW/TF-IDF | 빈도 기반, 희소 | 문서 분류 |\n",
    "| Word2Vec | 의미 기반, 밀집 | 유사도, 감성 |\n",
    "\n",
    "---\n",
    "\n",
    "### 다음 차시 예고\n",
    "- 26차시: [실습] 금융 뉴스 감성 분석\n",
    "  - 네이버 금융 뉴스 크롤링 (모듈 2 연계)\n",
    "  - 감성 사전 기반 분석\n",
    "  - 뉴스 감성과 주가 관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7eb21",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

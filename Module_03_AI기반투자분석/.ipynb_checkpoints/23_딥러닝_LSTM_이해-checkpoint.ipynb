{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ecda57",
   "metadata": {},
   "source": [
    "# 23차시: 딥러닝 기초 - 시계열 예측 모델 LSTM 이해하기\n",
    "\n",
    "## 학습 목표\n",
    "- 딥러닝과 신경망의 기본 개념 이해\n",
    "- RNN (Recurrent Neural Network)의 원리 학습\n",
    "- LSTM (Long Short-Term Memory)의 구조와 장점 이해\n",
    "- TensorFlow/Keras 기초 사용법 익히기\n",
    "\n",
    "## 학습 내용\n",
    "1. 딥러닝 기초\n",
    "2. RNN의 원리\n",
    "3. LSTM 구조\n",
    "4. TensorFlow/Keras 기초\n",
    "5. 간단한 LSTM 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db7e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca59a37",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. 딥러닝 기초\n",
    "\n",
    "### 딥러닝이란?\n",
    "**딥러닝(Deep Learning)**: 여러 층의 인공신경망을 사용하여 복잡한 패턴을 학습하는 머신러닝의 한 분야\n",
    "\n",
    "### 머신러닝 vs 딥러닝\n",
    "| 구분 | 머신러닝 | 딥러닝 |\n",
    "|------|----------|--------|\n",
    "| 특성 추출 | 수동 (사람이 설계) | 자동 (모델이 학습) |\n",
    "| 데이터 양 | 적은 데이터도 가능 | 대량 데이터 필요 |\n",
    "| 계산 자원 | CPU 가능 | GPU 권장 |\n",
    "| 해석력 | 높음 | 낮음 (블랙박스) |\n",
    "\n",
    "### 신경망 구조\n",
    "```\n",
    "입력층          은닉층(들)         출력층\n",
    "[x1] ─┐      ┌─[h1]─┐      ┌─[y1]\n",
    "[x2] ─┼──────┼─[h2]─┼──────┤\n",
    "[x3] ─┘      └─[h3]─┘      └─[y2]\n",
    "\n",
    "활성화 함수: ReLU, Sigmoid, Tanh 등\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f4b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 시각화\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# 층 위치\n",
    "layers = [0, 1, 2]\n",
    "layer_names = ['Input\\n(Features)', 'Hidden\\n(Processing)', 'Output\\n(Prediction)']\n",
    "neurons = [3, 4, 1]\n",
    "\n",
    "# 노드 그리기\n",
    "for i, (layer_x, n_neurons) in enumerate(zip(layers, neurons)):\n",
    "    for j in range(n_neurons):\n",
    "        y = (n_neurons - 1) / 2 - j\n",
    "        circle = plt.Circle((layer_x, y), 0.15, color='steelblue', ec='black', lw=2)\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        # 연결선 (다음 층으로)\n",
    "        if i < len(layers) - 1:\n",
    "            for k in range(neurons[i+1]):\n",
    "                next_y = (neurons[i+1] - 1) / 2 - k\n",
    "                ax.plot([layer_x + 0.15, layers[i+1] - 0.15], [y, next_y], \n",
    "                       'gray', alpha=0.3, lw=0.5)\n",
    "\n",
    "# 층 이름\n",
    "for layer_x, name in zip(layers, layer_names):\n",
    "    ax.text(layer_x, -2.5, name, ha='center', fontsize=11)\n",
    "\n",
    "ax.set_xlim(-0.5, 2.5)\n",
    "ax.set_ylim(-3, 2)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Simple Neural Network Structure', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e645b8",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. RNN (Recurrent Neural Network)의 원리\n",
    "\n",
    "### 왜 시계열에 RNN?\n",
    "- 일반 신경망은 입력 간 **순서**를 고려하지 않음\n",
    "- 주가는 **시간 순서**가 중요 (어제 가격 → 오늘 가격 → 내일 가격)\n",
    "- RNN은 **이전 시점의 정보를 기억**하면서 학습\n",
    "\n",
    "### RNN 구조\n",
    "```\n",
    "시간 t-2      시간 t-1      시간 t\n",
    "[x(t-2)] → [h(t-2)] → [x(t-1)] → [h(t-1)] → [x(t)] → [h(t)] → [y]\n",
    "              ↑                      ↑                   ↑\n",
    "          이전 정보              이전 정보           최종 출력\n",
    "          (hidden state)\n",
    "```\n",
    "\n",
    "### RNN의 한계: 장기 의존성 문제\n",
    "- 오래 전 정보가 점점 희미해짐 (Vanishing Gradient)\n",
    "- 예: 30일 전 주가가 오늘에 영향을 미치기 어려움\n",
    "- **해결책 → LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8285e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 구조 시각화\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# 시간 단계\n",
    "time_steps = ['t-2', 't-1', 't']\n",
    "x_positions = [0, 2, 4]\n",
    "\n",
    "for i, (t, x_pos) in enumerate(zip(time_steps, x_positions)):\n",
    "    # 입력\n",
    "    ax.annotate(f'x({t})', (x_pos, 0), ha='center', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black'))\n",
    "    \n",
    "    # 은닉 상태\n",
    "    ax.annotate(f'h({t})', (x_pos, 1), ha='center', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black'))\n",
    "    \n",
    "    # 입력 → 은닉\n",
    "    ax.annotate('', xy=(x_pos, 0.7), xytext=(x_pos, 0.3),\n",
    "                arrowprops=dict(arrowstyle='->', color='black'))\n",
    "    \n",
    "    # 은닉 → 다음 은닉\n",
    "    if i < len(time_steps) - 1:\n",
    "        ax.annotate('', xy=(x_positions[i+1] - 0.3, 1), xytext=(x_pos + 0.3, 1),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "        ax.text((x_pos + x_positions[i+1]) / 2, 1.3, 'Memory', ha='center', \n",
    "                fontsize=9, color='red')\n",
    "\n",
    "# 출력\n",
    "ax.annotate('y', (x_positions[-1], 2), ha='center', fontsize=11,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black'))\n",
    "ax.annotate('', xy=(x_positions[-1], 1.7), xytext=(x_positions[-1], 1.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='black'))\n",
    "\n",
    "ax.set_xlim(-1, 5)\n",
    "ax.set_ylim(-0.5, 2.5)\n",
    "ax.axis('off')\n",
    "ax.set_title('RNN: Recurrent Neural Network', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad48580",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. LSTM (Long Short-Term Memory)\n",
    "\n",
    "### LSTM이란?\n",
    "- RNN의 장기 의존성 문제를 해결한 모델\n",
    "- **게이트(Gate)** 메커니즘으로 정보를 선택적으로 기억/망각\n",
    "- 시계열 예측(주가, 날씨 등)에 널리 사용\n",
    "\n",
    "### LSTM 게이트 구조\n",
    "```\n",
    "1. Forget Gate (망각 게이트): 이전 정보 중 버릴 것 결정\n",
    "2. Input Gate (입력 게이트): 새 정보 중 저장할 것 결정\n",
    "3. Output Gate (출력 게이트): 현재 출력할 것 결정\n",
    "4. Cell State: 장기 기억 저장소\n",
    "```\n",
    "\n",
    "### LSTM vs RNN\n",
    "| 구분 | RNN | LSTM |\n",
    "|------|-----|------|\n",
    "| 장기 기억 | 어려움 | 가능 |\n",
    "| 구조 | 단순 | 복잡 (게이트) |\n",
    "| 학습 속도 | 빠름 | 느림 |\n",
    "| 성능 | 짧은 시퀀스 | 긴 시퀀스에 우수 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5bd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 게이트 개념 시각화\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# LSTM 셀\n",
    "cell_box = plt.Rectangle((1, 0.5), 4, 3, fill=True, facecolor='lightyellow', \n",
    "                          edgecolor='black', linewidth=2)\n",
    "ax.add_patch(cell_box)\n",
    "ax.text(3, 3.7, 'LSTM Cell', ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 게이트\n",
    "gates = [\n",
    "    ('Forget\\nGate', 1.5, 2, 'lightcoral'),\n",
    "    ('Input\\nGate', 2.8, 2, 'lightgreen'),\n",
    "    ('Output\\nGate', 4.1, 2, 'lightblue'),\n",
    "]\n",
    "\n",
    "for name, x, y, color in gates:\n",
    "    gate = plt.Rectangle((x, y), 0.8, 1, facecolor=color, edgecolor='black')\n",
    "    ax.add_patch(gate)\n",
    "    ax.text(x + 0.4, y + 0.5, name, ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Cell State\n",
    "ax.annotate('Cell State (Long-term Memory)', (3, 1.2), ha='center', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', edgecolor='gray'))\n",
    "ax.annotate('', xy=(4.8, 1.2), xytext=(1.2, 1.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='purple', lw=3))\n",
    "\n",
    "# 입출력\n",
    "ax.annotate('x(t)\\n(Input)', (0.3, 2), ha='center', fontsize=10)\n",
    "ax.annotate('h(t)\\n(Output)', (5.7, 2), ha='center', fontsize=10)\n",
    "ax.annotate('h(t-1)\\n(Previous)', (0.3, 3.2), ha='center', fontsize=9)\n",
    "ax.annotate('', xy=(1, 2.5), xytext=(0.5, 2.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='black'))\n",
    "ax.annotate('', xy=(5.5, 2.5), xytext=(5, 2.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='black'))\n",
    "\n",
    "ax.set_xlim(0, 6)\n",
    "ax.set_ylim(0, 4.5)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e8e9f",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. TensorFlow/Keras 기초\n",
    "\n",
    "### Keras란?\n",
    "- TensorFlow 위에서 동작하는 고수준 딥러닝 API\n",
    "- 직관적이고 간결한 코드로 모델 구축 가능\n",
    "\n",
    "### Keras 기본 패턴\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# 1. 모델 구조 정의\n",
    "model = Sequential([\n",
    "    LSTM(units=50, input_shape=(시퀀스길이, 특성수)),\n",
    "    Dense(units=1)\n",
    "])\n",
    "\n",
    "# 2. 컴파일\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 3. 학습\n",
    "model.fit(X_train, y_train, epochs=50)\n",
    "\n",
    "# 4. 예측\n",
    "y_pred = model.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8bf080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, SimpleRNN\n",
    "\n",
    "print(\"[TensorFlow/Keras 버전]\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 Keras 모델 예시\n",
    "print(\"[간단한 LSTM 모델 구조]\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model_example = Sequential([\n",
    "    LSTM(units=32, input_shape=(10, 1), return_sequences=False),  # 10일, 1특성\n",
    "    Dense(units=16, activation='relu'),\n",
    "    Dense(units=1)  # 출력: 1개 값 (다음날 주가)\n",
    "])\n",
    "\n",
    "model_example.compile(optimizer='adam', loss='mse')\n",
    "model_example.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5271e21f",
   "metadata": {},
   "source": [
    "### 주요 파라미터\n",
    "| 파라미터 | 설명 |\n",
    "|----------|------|\n",
    "| `units` | LSTM 유닛(뉴런) 수 |\n",
    "| `input_shape` | (시퀀스 길이, 특성 수) |\n",
    "| `return_sequences` | True: 모든 시점 출력 / False: 마지막만 |\n",
    "| `activation` | 활성화 함수 (tanh, relu 등) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff739f",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. 간단한 LSTM 예제\n",
    "\n",
    "사인파(Sin Wave) 데이터로 LSTM의 시계열 예측 능력을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef8f9cb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 사인파 데이터 생성\n",
    "np.random.seed(42)\n",
    "\n",
    "# 시간 축\n",
    "t = np.linspace(0, 100, 1000)\n",
    "# 사인파 + 노이즈\n",
    "data = np.sin(t * 0.5) + np.random.randn(1000) * 0.1\n",
    "\n",
    "print(\"[사인파 데이터]\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"데이터 길이: {len(data)}\")\n",
    "\n",
    "# 시각화\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(t[:200], data[:200], linewidth=1)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Sin Wave with Noise (First 200 points)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스 데이터 생성 함수\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    시계열 데이터를 LSTM 입력 형태로 변환\n",
    "    seq_length 개의 과거 데이터로 다음 값을 예측\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 시퀀스 길이 설정\n",
    "SEQ_LENGTH = 20  # 과거 20개 데이터로 다음 값 예측\n",
    "\n",
    "X, y = create_sequences(data, SEQ_LENGTH)\n",
    "X = X.reshape(-1, SEQ_LENGTH, 1)  # (samples, timesteps, features)\n",
    "\n",
    "print(\"[시퀀스 데이터 생성]\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"X shape: {X.shape} (샘플수, 시퀀스길이, 특성수)\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습/테스트 분할\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"학습 데이터: {len(X_train)}개\")\n",
    "print(f\"테스트 데이터: {len(X_test)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbcce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 구축\n",
    "model = Sequential([\n",
    "    LSTM(units=32, input_shape=(SEQ_LENGTH, 1)),\n",
    "    Dense(units=1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(\"[LSTM 모델]\")\n",
    "print(\"=\" * 50)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbfd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "print(\"\\n[모델 학습]\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3975e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "ax.plot(history.history['loss'], label='Train Loss')\n",
    "ax.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "ax.set_title('Training History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d51e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 및 평가\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "mse = np.mean((y_test - y_pred) ** 2)\n",
    "print(f\"[테스트 결과] MSE: {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 시각화\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# 전체 비교\n",
    "axes[0].plot(y_test, label='Actual', alpha=0.7)\n",
    "axes[0].plot(y_pred, label='Predicted', alpha=0.7)\n",
    "axes[0].set_xlabel('Sample')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].set_title('LSTM Prediction: Actual vs Predicted')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 일부 확대\n",
    "axes[1].plot(y_test[:100], label='Actual', linewidth=2)\n",
    "axes[1].plot(y_pred[:100], label='Predicted', linewidth=2, linestyle='--')\n",
    "axes[1].set_xlabel('Sample')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('LSTM Prediction: First 100 Samples (Zoomed)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490623a2",
   "metadata": {},
   "source": [
    "---\n",
    "## 학습 정리\n",
    "\n",
    "### 1. RNN과 LSTM\n",
    "| 구분 | RNN | LSTM |\n",
    "|------|-----|------|\n",
    "| 장기 기억 | 어려움 | 가능 (Cell State) |\n",
    "| 게이트 | 없음 | Forget/Input/Output |\n",
    "| 사용처 | 짧은 시퀀스 | 긴 시퀀스, 주가 예측 |\n",
    "\n",
    "### 2. Keras 기본 패턴\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(units=50, input_shape=(seq_len, features)),\n",
    "    Dense(units=1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=50)\n",
    "```\n",
    "\n",
    "### 3. 데이터 준비\n",
    "- 시퀀스 형태로 변환: `(samples, timesteps, features)`\n",
    "- 정규화 권장 (0~1 또는 표준화)\n",
    "\n",
    "### 4. 주의사항\n",
    "- 충분한 데이터 필요\n",
    "- 과적합 주의 (Dropout 활용)\n",
    "- 하이퍼파라미터 튜닝 중요\n",
    "\n",
    "---\n",
    "\n",
    "### 다음 차시 예고\n",
    "- 24차시: [실습] LSTM 모델을 활용한 주가 예측\n",
    "  - 실제 삼성전자 주가 데이터 사용\n",
    "  - 데이터 전처리 및 스케일링\n",
    "  - 모델 학습 및 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ef6d2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

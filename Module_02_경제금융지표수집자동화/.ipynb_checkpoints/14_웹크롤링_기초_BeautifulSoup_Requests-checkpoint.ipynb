{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee610211",
   "metadata": {},
   "source": [
    "# 14차시: 웹 크롤링 기초 - BeautifulSoup과 Requests\n",
    "\n",
    "## 학습 목표\n",
    "- API가 제공되지 않는 웹사이트의 정보(뉴스, 시장 지표)를 파이썬으로 수집하는 원리 이해\n",
    "- Requests, BeautifulSoup 라이브러리의 기본 사용법 학습\n",
    "- HTML 문서의 구조를 이해하고 원하는 데이터를 추출하는 방법 습득\n",
    "\n",
    "## 학습 내용\n",
    "1. 웹 크롤링 개념 및 API와의 차이점\n",
    "2. HTML 기초 구조 이해\n",
    "3. Requests 라이브러리로 웹페이지 가져오기\n",
    "4. BeautifulSoup으로 HTML 파싱하기\n",
    "5. 실습: 간단한 웹페이지 파싱\n",
    "\n",
    "## 구분\n",
    "이론/실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037be524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치 (Colab)\n",
    "!pip install beautifulsoup4 requests -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40528c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"[라이브러리 로드 완료]\")\n",
    "print(f\"  - requests 버전: {requests.__version__}\")\n",
    "print(f\"  - BeautifulSoup: HTML 파싱 라이브러리\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7586195",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. 웹 크롤링 개념\n",
    "\n",
    "### API vs 웹 크롤링\n",
    "\n",
    "| 구분 | API | 웹 크롤링 |\n",
    "|------|-----|-----------|\n",
    "| **정의** | 서버가 제공하는 공식 데이터 인터페이스 | 웹페이지 HTML을 직접 파싱하여 데이터 추출 |\n",
    "| **데이터 형식** | JSON, XML 등 구조화된 형식 | HTML (비구조화) |\n",
    "| **안정성** | 높음 (공식 지원) | 낮음 (HTML 구조 변경 시 코드 수정 필요) |\n",
    "| **사용 예시** | DART API, FRED API (11-13차시) | 네이버 금융, 뉴스 사이트 |\n",
    "\n",
    "### 웹 크롤링이 필요한 경우\n",
    "- 공식 API가 제공되지 않는 경우\n",
    "- API에서 제공하지 않는 데이터가 필요한 경우\n",
    "- 여러 웹사이트의 데이터를 통합해야 하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1490184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API vs 웹 크롤링 비교 예시\n",
    "print(\"[API vs 웹 크롤링 비교]\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"[API 방식 - 11-13차시에서 학습]\")\n",
    "print(\"  DART API: 공시 데이터를 JSON 형식으로 제공\")\n",
    "print(\"  FRED API: 경제 지표를 JSON 형식으로 제공\")\n",
    "print(\"  장점: 데이터가 구조화되어 있어 파싱이 쉬움\")\n",
    "print()\n",
    "print(\"[웹 크롤링 방식 - 이번 차시]\")\n",
    "print(\"  네이버 금융: 환율, 뉴스 등을 HTML에서 직접 추출\")\n",
    "print(\"  장점: API가 없어도 웹에 보이는 모든 데이터 수집 가능\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b837d9",
   "metadata": {},
   "source": [
    "### 크롤링 시 주의사항\n",
    "\n",
    "1. **robots.txt 확인**: 웹사이트가 크롤링을 허용하는지 확인\n",
    "2. **서버 부하 고려**: 과도한 요청은 서버에 부담 (적절한 딜레이 필요)\n",
    "3. **저작권 준수**: 수집한 데이터의 상업적 사용 시 저작권 확인\n",
    "4. **이용약관 확인**: 해당 웹사이트의 이용약관 준수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da4153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# robots.txt 확인 방법\n",
    "print(\"[robots.txt 확인 방법]\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"웹사이트의 크롤링 정책은 robots.txt 파일에서 확인할 수 있습니다.\")\n",
    "print()\n",
    "\n",
    "# 네이버 금융 robots.txt 확인\n",
    "url = \"https://finance.naver.com/robots.txt\"\n",
    "try:\n",
    "    response = requests.get(url, timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"[{url}]\")\n",
    "        print(\"-\" * 60)\n",
    "        print(response.text[:500])\n",
    "        print(\"...\")\n",
    "except Exception as e:\n",
    "    print(f\"확인 실패: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5abc3c",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. HTML 기초 구조\n",
    "\n",
    "HTML(HyperText Markup Language)은 웹페이지의 구조를 정의하는 마크업 언어입니다.\n",
    "\n",
    "### HTML 기본 구성요소\n",
    "- **태그(Tag)**: `<태그명>내용</태그명>` 형식\n",
    "- **속성(Attribute)**: `<태그 속성=\"값\">` 형식\n",
    "- **계층 구조**: 부모-자식 관계로 중첩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76179e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML 구조 예시\n",
    "sample_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>금융 데이터 예시</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>오늘의 시장 지표</h1>\n",
    "    <div class=\"market-data\" id=\"exchange\">\n",
    "        <h2>환율</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>통화</th>\n",
    "                <th>환율</th>\n",
    "            </tr>\n",
    "            <tr class=\"usd\">\n",
    "                <td>USD/KRW</td>\n",
    "                <td>1,380.50</td>\n",
    "            </tr>\n",
    "            <tr class=\"jpy\">\n",
    "                <td>JPY/KRW</td>\n",
    "                <td>9.12</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "    <div class=\"news\" id=\"headlines\">\n",
    "        <h2>주요 뉴스</h2>\n",
    "        <ul>\n",
    "            <li><a href=\"/news/001\">코스피 3000 돌파</a></li>\n",
    "            <li><a href=\"/news/002\">삼성전자 실적 발표</a></li>\n",
    "            <li><a href=\"/news/003\">미국 금리 동결 전망</a></li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "print(\"[샘플 HTML 구조]\")\n",
    "print(\"=\" * 60)\n",
    "print(sample_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9aee92",
   "metadata": {},
   "source": [
    "### HTML 주요 태그\n",
    "\n",
    "| 태그 | 설명 | 예시 |\n",
    "|------|------|------|\n",
    "| `<div>` | 구획(Division) | `<div class=\"content\">...</div>` |\n",
    "| `<table>` | 표 | `<table><tr><td>...</td></tr></table>` |\n",
    "| `<a>` | 링크 | `<a href=\"url\">텍스트</a>` |\n",
    "| `<ul>`, `<li>` | 목록 | `<ul><li>항목</li></ul>` |\n",
    "| `<span>` | 인라인 요소 | `<span class=\"price\">1,000</span>` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c72895",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Requests 라이브러리\n",
    "\n",
    "Requests는 HTTP 요청을 보내고 응답을 받는 라이브러리입니다.\n",
    "\n",
    "### 기본 사용법\n",
    "```python\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eebf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requests 기본 사용법\n",
    "print(\"[Requests 기본 사용법]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 간단한 웹페이지 요청\n",
    "url = \"https://httpbin.org/get\"\n",
    "response = requests.get(url)\n",
    "\n",
    "print(f\"\\n요청 URL: {url}\")\n",
    "print(f\"응답 상태 코드: {response.status_code}\")\n",
    "print(f\"응답 헤더 Content-Type: {response.headers.get('Content-Type')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f42e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP 상태 코드\n",
    "print(\"\\n[HTTP 상태 코드]\")\n",
    "print(\"=\" * 60)\n",
    "print(\"  200: 성공 (OK)\")\n",
    "print(\"  301/302: 리다이렉션\")\n",
    "print(\"  403: 접근 금지 (Forbidden)\")\n",
    "print(\"  404: 페이지 없음 (Not Found)\")\n",
    "print(\"  500: 서버 오류\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent 헤더 설정\n",
    "print(\"\\n[User-Agent 헤더 설정]\")\n",
    "print(\"=\" * 60)\n",
    "print(\"일부 웹사이트는 봇의 접근을 차단합니다.\")\n",
    "print(\"User-Agent를 설정하면 브라우저처럼 접근할 수 있습니다.\")\n",
    "print()\n",
    "\n",
    "# 헤더 없이 요청\n",
    "headers_none = {}\n",
    "\n",
    "# 브라우저처럼 보이는 헤더\n",
    "headers_browser = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "print(\"[헤더 설정 예시]\")\n",
    "print(\"headers = {\")\n",
    "print(\"    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ...'\")\n",
    "print(\"}\")\n",
    "print()\n",
    "print(\"response = requests.get(url, headers=headers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf75514",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "BeautifulSoup은 HTML/XML 문서를 파싱하여 데이터를 추출하는 라이브러리입니다.\n",
    "\n",
    "### 기본 사용법\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c12a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup 기본 사용법\n",
    "print(\"[BeautifulSoup 기본 사용법]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 샘플 HTML 파싱\n",
    "soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "\n",
    "print(\"\\n[1] 타이틀 추출\")\n",
    "print(f\"  soup.title.text = '{soup.title.text}'\")\n",
    "\n",
    "print(\"\\n[2] 첫 번째 h1 태그\")\n",
    "print(f\"  soup.h1.text = '{soup.h1.text}'\")\n",
    "\n",
    "print(\"\\n[3] 첫 번째 div 태그\")\n",
    "div = soup.div\n",
    "print(f\"  soup.div['class'] = {div['class']}\")\n",
    "print(f\"  soup.div['id'] = '{div['id']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb438a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find() vs find_all()\n",
    "print(\"\\n[find() vs find_all()]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n[1] find(): 첫 번째 요소만 반환\")\n",
    "first_div = soup.find('div')\n",
    "print(f\"  soup.find('div')['id'] = '{first_div['id']}'\")\n",
    "\n",
    "print(\"\\n[2] find_all(): 모든 요소를 리스트로 반환\")\n",
    "all_divs = soup.find_all('div')\n",
    "print(f\"  soup.find_all('div') 개수: {len(all_divs)}\")\n",
    "for i, div in enumerate(all_divs):\n",
    "    print(f\"    [{i}] id='{div.get('id', 'N/A')}', class={div.get('class', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 속성으로 검색\n",
    "print(\"\\n[속성으로 검색]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n[1] id로 검색\")\n",
    "exchange_div = soup.find('div', id='exchange')\n",
    "print(f\"  soup.find('div', id='exchange')\")\n",
    "print(f\"  → h2: {exchange_div.h2.text}\")\n",
    "\n",
    "print(\"\\n[2] class로 검색\")\n",
    "usd_row = soup.find('tr', class_='usd')\n",
    "print(f\"  soup.find('tr', class_='usd')\")\n",
    "tds = usd_row.find_all('td')\n",
    "print(f\"  → 통화: {tds[0].text}, 환율: {tds[1].text}\")\n",
    "\n",
    "print(\"\\n[3] 여러 조건 결합\")\n",
    "news_div = soup.find('div', {'class': 'news', 'id': 'headlines'})\n",
    "print(f\"  soup.find('div', {{'class': 'news', 'id': 'headlines'}})\")\n",
    "print(f\"  → 찾음: {news_div is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트와 속성 추출\n",
    "print(\"\\n[텍스트와 속성 추출]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n[1] 텍스트 추출: .text 또는 .get_text()\")\n",
    "h1 = soup.find('h1')\n",
    "print(f\"  h1.text = '{h1.text}'\")\n",
    "print(f\"  h1.get_text() = '{h1.get_text()}'\")\n",
    "\n",
    "print(\"\\n[2] 링크(href) 추출\")\n",
    "links = soup.find_all('a')\n",
    "print(\"  뉴스 링크:\")\n",
    "for link in links:\n",
    "    print(f\"    제목: {link.text}, URL: {link['href']}\")\n",
    "\n",
    "print(\"\\n[3] 특정 속성 추출: ['속성명'] 또는 .get('속성명')\")\n",
    "first_link = soup.find('a')\n",
    "print(f\"  first_link['href'] = '{first_link['href']}'\")\n",
    "print(f\"  first_link.get('href') = '{first_link.get('href')}'\")\n",
    "print(f\"  first_link.get('target', 'N/A') = '{first_link.get('target', 'N/A')}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc86b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS 선택자 사용\n",
    "print(\"\\n[CSS 선택자 - select()]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n[1] 태그 선택\")\n",
    "items = soup.select('li')\n",
    "print(f\"  soup.select('li') 개수: {len(items)}\")\n",
    "\n",
    "print(\"\\n[2] 클래스 선택 (.클래스명)\")\n",
    "market_data = soup.select('.market-data')\n",
    "print(f\"  soup.select('.market-data') 개수: {len(market_data)}\")\n",
    "\n",
    "print(\"\\n[3] ID 선택 (#id명)\")\n",
    "headlines = soup.select('#headlines')\n",
    "print(f\"  soup.select('#headlines') 개수: {len(headlines)}\")\n",
    "\n",
    "print(\"\\n[4] 계층 구조 선택\")\n",
    "news_links = soup.select('#headlines ul li a')\n",
    "print(f\"  soup.select('#headlines ul li a'):\")\n",
    "for link in news_links:\n",
    "    print(f\"    → {link.text}\")\n",
    "\n",
    "print(\"\\n[5] 첫 번째 요소만: select_one()\")\n",
    "first_news = soup.select_one('#headlines li a')\n",
    "print(f\"  soup.select_one('#headlines li a') = '{first_news.text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d12f56",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. 실습: 테이블 데이터 추출\n",
    "\n",
    "HTML 테이블에서 데이터를 추출하여 DataFrame으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22846e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테이블 데이터 추출\n",
    "print(\"[테이블 데이터 추출]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 테이블 찾기\n",
    "table = soup.find('table')\n",
    "\n",
    "# 헤더 추출\n",
    "headers = []\n",
    "header_row = table.find('tr')\n",
    "for th in header_row.find_all('th'):\n",
    "    headers.append(th.text)\n",
    "print(f\"\\n헤더: {headers}\")\n",
    "\n",
    "# 데이터 행 추출\n",
    "data = []\n",
    "data_rows = table.find_all('tr')[1:]  # 헤더 제외\n",
    "for row in data_rows:\n",
    "    cols = row.find_all('td')\n",
    "    row_data = [col.text for col in cols]\n",
    "    data.append(row_data)\n",
    "print(f\"데이터: {data}\")\n",
    "\n",
    "# DataFrame 변환\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "print(\"\\n[DataFrame으로 변환]\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4dfb61",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. 실습: 실제 웹페이지 크롤링 맛보기\n",
    "\n",
    "네이버 금융 메인 페이지에 접속해서 HTML 구조를 확인해봅니다.\n",
    "(다음 차시에서 본격적으로 데이터를 추출합니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ff7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 금융 메인 페이지 접속\n",
    "print(\"[네이버 금융 메인 페이지 접속]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "url = \"https://finance.naver.com/\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    print(f\"\\n상태 코드: {response.status_code}\")\n",
    "    print(f\"인코딩: {response.encoding}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # 페이지 타이틀\n",
    "        title = soup.title.text if soup.title else \"N/A\"\n",
    "        print(f\"페이지 타이틀: {title}\")\n",
    "        \n",
    "        # div 태그 개수\n",
    "        divs = soup.find_all('div')\n",
    "        print(f\"div 태그 개수: {len(divs)}개\")\n",
    "        \n",
    "        # 링크 개수\n",
    "        links = soup.find_all('a')\n",
    "        print(f\"링크(a 태그) 개수: {len(links)}개\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"요청 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb35089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 금융 HTML 구조 살펴보기\n",
    "print(\"\\n[네이버 금융 HTML 구조 미리보기]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 주요 섹션 찾기\n",
    "    print(\"\\n[주요 ID를 가진 요소들]\")\n",
    "    important_ids = ['container', 'content', 'market', 'news']\n",
    "    for id_name in important_ids:\n",
    "        elem = soup.find(id=id_name)\n",
    "        if elem:\n",
    "            print(f\"  #{id_name}: {elem.name} 태그\")\n",
    "    \n",
    "    # 클래스에 'market' 포함된 요소\n",
    "    print(\"\\n[클래스에 'market' 포함된 요소]\")\n",
    "    market_elements = soup.find_all(class_=lambda x: x and 'market' in x.lower() if x else False)\n",
    "    for elem in market_elements[:5]:\n",
    "        classes = elem.get('class', [])\n",
    "        print(f\"  <{elem.name}> class={classes}\")\n",
    "    \n",
    "    print(\"\\n다음 차시에서 이 구조를 분석하여 환율, 뉴스 등을 추출합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dae98b",
   "metadata": {},
   "source": [
    "---\n",
    "## 학습 정리\n",
    "\n",
    "### 1. 웹 크롤링 개념\n",
    "- API가 없는 웹사이트에서 데이터를 수집하는 기술\n",
    "- HTML 문서를 파싱하여 원하는 정보 추출\n",
    "- robots.txt 및 이용약관 확인 필요\n",
    "\n",
    "### 2. Requests 라이브러리\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# 기본 요청\n",
    "response = requests.get(url)\n",
    "\n",
    "# User-Agent 헤더 설정\n",
    "headers = {'User-Agent': 'Mozilla/5.0 ...'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "response.status_code  # 200이면 성공\n",
    "response.text         # HTML 내용\n",
    "```\n",
    "\n",
    "### 3. BeautifulSoup 핵심 메서드\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 요소 찾기\n",
    "soup.find('태그')           # 첫 번째 요소\n",
    "soup.find_all('태그')       # 모든 요소 (리스트)\n",
    "soup.find('태그', id='값')   # id로 검색\n",
    "soup.find('태그', class_='값')  # class로 검색\n",
    "\n",
    "# CSS 선택자\n",
    "soup.select('선택자')       # 모든 요소 (리스트)\n",
    "soup.select_one('선택자')   # 첫 번째 요소\n",
    "\n",
    "# 데이터 추출\n",
    "element.text              # 텍스트 내용\n",
    "element['속성']           # 속성 값\n",
    "element.get('속성', 기본값)  # 속성 값 (없으면 기본값)\n",
    "```\n",
    "\n",
    "### 4. 주요 CSS 선택자\n",
    "| 선택자 | 의미 | 예시 |\n",
    "|--------|------|------|\n",
    "| `태그` | 태그명 | `div`, `a`, `table` |\n",
    "| `.클래스` | 클래스 | `.market-data` |\n",
    "| `#아이디` | ID | `#headlines` |\n",
    "| `부모 자식` | 계층 구조 | `div ul li a` |\n",
    "\n",
    "---\n",
    "\n",
    "### 다음 차시 예고\n",
    "- 15차시: [실습] 네이버 금융에서 뉴스 타이틀과 시장 지표 크롤링\n",
    "  - 환율, 유가, 주가지수 추출\n",
    "  - 뉴스 헤드라인 수집\n",
    "  - 종목 재무정보 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8df61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
